---
title: "HEQCO Learning Outcomes Project: `r params$dept_title`"
author: "Jake Kaupp"
date: "`r Sys.Date()`"
output: rmarkdown::tufte_handout
params:
  dept:
    value: PSYC
  dept_title:
    value: Department of Psychology
---
```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
source("/Users/Jake/ownCloud/Projects/R/Projects/LOAC/R/report_data.R")
opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

```{r p_select}
if(params$dept=="ECE")
{
p.select <- c("ELEC","CMPE")
} else {
p.select <- params$dept
}
```

\vspace{40 mm}

\newthought{The Higher Education Quality Council}[^heqco], created a consortium of three Universities and thee Colleges in Ontario, and tasked them with determining methods for assessing broad, high-level learning outcomes across an institution.  The Queen's Learning Outcomes Project[^qlop], is a four-year longitudinal study that is looking at methods to assess cognitive skills, such as Critical Thinking, Problem Solving, Written Communication and Lifelong Learning, in the Faculty of Arts and Science and the Faculty of Engineering and Applied Science.  There are a number of participating departments in both Faculties:

* Department of Drama
* Department of Psychology
* Department of Physics
* All 8 Departments in the Faculty of Engineering and Applied Science.

The project tracks the 2013-2014 cohort of students as they progress through their undergraduate experience at Queen's, to investigate how students develop these skills over the duration of their undegradaute education and to determine the efficacy, efficiency and sustainability of each assessment method. 

The results from the project offer a means of triangulating results from other assessment instruments, such as the National Survey of Student Engagement or Queen's Graduate Exit Survey, and offer evidence towards demonstrating student performance in CEAB's graduate attributes or the Council of Ontario Universities Degree Level Expectations.  This document presents a brief summary of each of the instruments, and a summary of the results for the `r params$dept_title`.

\newpage

# Assessment Tools

The Queen's Learning Outcomes Project is using four tools to assess cognitive skills:

* Standarized Instruments (The Collegiate Learning Assessment Plus and the Critical Thinking Assessment Test)
* Program level meta-rubrics (The Valid Assessment of Learning in Undergradate Education or VALUE Rubrics)
* Qualitiative Surveys (The Transferrable Learning Orientations Survey)

## The Collegiate Learning Assessment Plus (CLA+)[^cla+] 

The CLA+ is a 90 minute, standardized test that students take online.  The CLA+ is divided into two main tasks, each which are designed to measure a specific skills and abilities.  The first is a Performance task, in which students are placed in a scenario and are given a library of evidence.  They are asked to thoroughly read the evidence and then prepare their reponse for the scenario, which often has them support or refute a position or provide recommendations for potential action.  The second section of the CLA+ is a series of Selected-response questions, in which students are presented with a prompt and then asked to select a suitable respons from the options provided.  The questions ask students to evaluate claims, critique arguments, and interpret information. 

## Interpreting CLA+ Scores
The instrument is machine-scored, supplemented and checked by expert raters.  The results are presented as a set of absolute criterion referenced scores and subscores as well as a set of norm-referenced Mastery levels[^mastery].\marginnote{\textbf{Criterion-referenced} refers to tests and assessments  designed to measure student performance against a fixed set of predetermined criteria or learning standards.}

\marginnote{\textbf{Norm-referenced} refers to standardized tests that are designed to compare and rank test takers in relation to one another.}

To better understand how to interpret the scores, consider the following.  Think of the absolute scores (Total Score, Performance Task Score, Selected Response Score) as the comparitive measure of the student to use their skills in concert, in a realistic setting.  The Mastery levels describe the characteristics and abilities, while the sub-scale scores measure how well the student performs in each specific skill.  You can have a student display excellent analytical reasoning and problem sovling, but poor written communication.  This would be reflected by a high sub-scale score for analytical reasoning, poor for writing mechanics and effectiveness; producing a moderate Performance Task Score and lowering the overall total score.  


# The Critical Thinking Assessment Test (CAT Test)[^cat]

The Critical Thinking Assessment Test (CAT) is a 60 minute, paper test that was collaboratively developed by Tennessee Technological University (TTU) and faculty from participating institutions. The primary goal of this collaboration was to develop a faculty-driven assessment tool to engage the faculty in meaningful, authentic assessment with the goal of improving student learning. The CAT assesses a consensus core set of five skills that address elements of critical thinking, problem solving, and communication across a variety of disciplines. 

The CAT consists of 15 questions requiring both quick response items (multiple choice, binary scale) and short answer essay responses that are likely to measure both the cognitive elements of critical thinking as well as the disposition of students to actually engage in critical thinking. 

## Interpreting CAT Scores
The CAT test is collaboratively scored by a group of trained raters, with the creators of the test endorsing a 'train-the-trainer' system following a detailed scoring guide.  While the creators of the test encourage faculty-led scoring sessions, to help faculty better understand the strenghts and weakeness of their students, all scoring sesssions at Queen's were conducted by expertly trained upper-year students.
\marginnote{In both the CAT and CLA+ the solitary scores are important, but the difference measured over time of greater value.}
The CAT scores are provided on a criterion referenced absolute scale, with a maximum possible score of 38.  Solitary scores are best interpreted holistically, with higher scores indicating a student that has a greater command of these skills and applies them in concert.  

# The Valid Assessment of Learning in Undergraduate Education (VALUE) Rubrics[^value]

Developed by the American Association of Colleges and Universities to provide a valid assessment of learning in undergraduate education.  These rubrics are broad, discipline-neutral descriptions of selected essential learning outcomes of undergraduate education from the Liberal Education America’s Promise (LEAP) initiative.  In each rubric, common themes were identified for each outcome and performance criteria were developed by panels of experts. The efforts of the experts were focused on:

* Performance criteria focuses on positive demonstration of outcomes rather than what was lacking
* Performance criteria can be used to assess to non-traditional modes of artefacts demonstrating student learning
* Performance criteria are developed to assess summative displays of student learning rather than developmental or formative displays
* Performance criteria are phrased in a manner as to be easily understood by non-experts

There are four levels of performance criteria, from the benchmark level of a student entering university to the capstone level of a student who has just completed their undergraduate experience.   While the performance criteria and levels represent a consensus of experts and can be used in their original form, the rubrics are purposely designed for modification to foster alignment between course, program or institutional outcomes and to reflect the specific context in which they are used. 

## Interpreting the VALUE Rubrics

The results illustrate the respective strengths and weaknesses of the cohort, measured through the evaluation of signature assessments from each year of the program.   \marginnote{While the results are measuring student performance, the assessment task can significantly influence the results. Assessments that are misaligned, or do not offer students the chance to display certain skills will bias results.} The assessents used in this project are deliverables from first and second year courses, these artifacts were selected in consultation with instructors and were thought to be the ones most suited for students to demonstrate the skills being assessed.  The results from the VALUE Rubrics are meant to be interpreted at the cohort level, which represents a certain demographic of a program or institution.  They are best viewed on a continumn, rather than an objective ranking or quantification of student ability.  In a way the results of the VALUE Rubrics can also be used to see how well an assessment task is suited for the development and assesment of essential learning outcomes.


# The Transferrable Learning Orientations Survey[^tlo]

The Transferrable Learning Orientations Survey (TLO) is a triangulated measure developed as an ongoing self-report measure of lifelong learning. It was built on select scales from the Motivated Strategies for Learning Questionnaire (MSLQ), together with multiple choice items adapted from the Association for American Colleges and Universities Lifelong Learning rubric, and an open-ended response for each dimension. The survey has three overarching factors, deep learning, self-belief, and learning strategies. The open-ended questions are designed to support meta-cognition; to engage students in thinking about their own learning.

The TLO was designed as a triangulated measure using a quantitative pairing of four scale items for each construct, correlated to a holistic rubric self-rating, with an open-ended response used for the purpose of feedback to the instructor and validation of final rating. The final levels are reported on a scale from one to four.  \marginnote{A score of one describes surface level engagement, fixed mindset, limited confidence and lack of organization, whereas four describes a deep learner, flexible mindset, confident, and high level of organization.}The 4 scale items are averaged and compared to the students self-rating on the rubric.  These two individual ratings are quite close, and may reflect students "true" and "ideal" self.  The open-ended resposnes then can be used to investigate differences and detemine which rating is more applicable. 

## Interpreting the Transferrable Learning Orientations Survey

The TLO self-report measure is not expected to represent a summative point for transferable, but rather part of an ongoing process.  The TLO is intended to be used as a self-assessment; administered before and after a course of study to determine if teaching and learning had an impact on approaches to lifelong learning. The TLO accomplishes this by asking students to think back over their past year and frame their responses in that fashion.  Interpreting TLO results at the program level can provide insight into how aspects of the program already support students in these aspects, how programming can be developed to better support students, and the student voice as to how the curriculum is shaping their approach towards learning.

\newpage

# Results

## CLA + Results

## Absolute Scores
Presented below are the combined results for the `r params$dept_title`.\marginnote{The average CLA+ score for a graduating senior from an american institution in 2013-2014 was 1128.}  First is the overall CLA+ score, which presents an indicator of the overall ability of students at each year, as well as the growth that occurs between years in the program. In total there `r CLA %>% filter(discipline %in% p.select,!is.na(score_total), project_year == 1) %>% tally` `r params$dept_title` students that took the CLA+ in the first year, with `r CLA %>% filter(discipline %in% p.select,!is.na(score_total), project_year == 2) %>% tally` `r params$dept_title` students taking the test in the second year.  Of that group, there were `r intersect(subset(CLA, discipline %in% p.select & project_year ==1 & !is.na(score_total), select = studentid), subset(CLA, discipline %in% p.select & project_year == 2 & !is.na(score_total), select = studentid)) %>% tally` students that took the test in both years. In the chart below, each student that took the CLA+ in consecutive years is plotted as a grey line connecting their score in each year.  The blue line represents the mean score, represented by the corhort mean in first and second year.

```{r CLA_total, fig.width = 12, fig.height = 6, fig.cap = paste0(params$dept_title, ": CLA+ Total Score"), fig.fullwidth = TRUE}
library(magrittr)
library(dplyr)
library(ggplot2)
library(ggthemes)

 CLA %>%
  group_by(studentid) %>% 
   filter(discipline %in% p.select,!is.na(score_total)) %>%
   mutate(project_year = as.numeric(project_year)) %>% 
   arrange(desc(project_year,score_total)) %>%
   ggplot(aes(x = project_year, y = score_total)) +
   geom_line(aes(group = studentid), alpha = 0.2) +
   stat_summary(
   aes(group = 1), size = 1, color = "blue" ,geom = "line", fun.y = "mean"
   ) +
   stat_summary(
   aes(label = floor(..y..)), fun.y = "mean", geom = "text", color = "blue", size = 4, vjust = -0.5
   ) +
   geom_rangeframe() +
   scale_x_continuous(limits = c(1,2), breaks = c(1,2), labels = c("2013-2014","2014-2015") ) +
   ylab("CLA+ Score") +
   xlab("Year") +
   theme_tufte(base_size = 16) +
   theme(axis.title.y = element_text(angle = 0))
  
  
```

\newpage

## Mastery Levels
Alternatively, the results can also be viewed by looking at the distribution of mastery levels at each year, illustrated below in histograms by each year. 
```{r CLA_Mastery, fig.width = 10, fig.height = 5, fig.cap = "CLA+ Mastery Score Distribution",  fig.fullwidth = TRUE}
suppressPackageStartupMessages(library(gdata, stringr))


data <- CLA %>%
   filter(discipline %in% p.select, !is.na(mastery)) %>% 
   group_by(studentid) %>% 
   filter(n()>1) %>% 
   mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015")),
             mastery = reorder.factor(mastery,new.order=c("Below Basic","Basic","Proficient","Advanced")))


max <- data %>% 
  group_by(project_year, mastery) %>% 
  tally %>%
  ungroup %>% 
  select(n) %>% 
  max()
  
  
ggplot(data, aes(x = mastery)) +
      geom_bar(fill = "grey50", width=0.5) +
      geom_hline(yintercept=seq(0, max, 5), col="white", lwd=1) +
      scale_x_discrete(labels = function(x) str_wrap(x, width = 5)) +
      geom_rangeframe() +
      facet_wrap(~project_year) +
      ylab("Count") +
      xlab("\nMastery Level") +
      geom_rangeframe() +
      theme_tufte(base_size = 16, ticks=F) +
      theme(axis.title.y = element_text(angle = 0))

```

## Confounding Factors
There are variety of factors that influence student performance on the CLA+.  This test was embedded within a course, but their performance on the test did not affect their course grades.  They were simply taking the test for participation credit.  This does introduce questions regarding the effort students put into the test, and their scores not reflecting their true ability if they don't put full effort into the test.  There are noted correlations between scores, effort and time on task, shown in the correlation heatmap on the next page.

\newpage

```{r Correlations, fig.width = 12, fig.height = 8,fig.cap = "Effort, Time and Score Correlations"}
cor1 <- CLA %>%
  filter(discipline %in% p.select) %>%
  filter(project_year == 1) %>% 
  select(score_total, score_pt, score_sr, effort_pt, effort_sr, time_pt, time_sr) %>%
  cor() %>% 
  reshape2::melt() %>% 
  mutate(project_year ="2013-2014")

cor2 <-  CLA %>%
  filter(discipline %in% p.select) %>%
  filter(project_year == 2) %>% 
  select(score_total, score_pt, score_sr, effort_pt, effort_sr, time_pt, time_sr) %>%
  cor() %>% 
  reshape2::melt() %>% 
  mutate(project_year = "2014-2015")

bind_rows(cor1,cor2) %>% 
      ggplot(aes(x = Var1, y = Var2)) +
      geom_tile(aes(fill = value), color = "black") +
      facet_wrap(~project_year) +
      ylab("") +
      xlab("\nCLA+ Varibles: Score, Effort and Time") +
      coord_equal(ratio = 1) +
      geom_rangeframe() +
      scale_fill_distiller(name = "Pearsons r", breaks=c(0,0.25,0.5,0.75,1), palette = "Greys") +
      theme_tufte(base_size = 18) +
      theme(legend.position = "bottom")

```

The CLA+ does address this by measuring effort and time on task, which allows for some measure of control and interpretation of score, the results of which are shown below.  It should be noted that there is a difference in time allotted to tasks, with 60 minutes alloted to the performance task and 30 minutes alloted to the selected response questions. 

```{r effort, fig.cap = "Self-reported Effort on Task", fig.width = 10, fig.height = 6}

data <- CLA %>%
  filter(discipline %in% p.select) %>% 
  mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>% 
  select(discipline, project_year,effort_pt,effort_sr) %>%
  rename(`Performance Task Effort` = effort_pt,
         `Selected Response Effort` = effort_sr) %>% 
  gather(Task,Value, -project_year, -discipline) %>% 
  filter(!is.na(Value))


max <- data %>%
  group_by(project_year, Task,Value) %>% tally %>% ungroup %>% select(n) %>% max() 
  
ggplot(data, aes(x = factor(Value))) +
  geom_bar(aes(fill = project_year), position = position_dodge()) +
  xlab("Self-reported\nEffort\n") +
  ylab("\nCount") +
  geom_hline(yintercept = seq(0, max, 5), col="white", lwd=1) +
  coord_flip() + 
  facet_grid(.~Task, labeller = label_wrap_gen(width=10)) +
  theme_tufte(base_size = 20) +
  theme(legend.position = "bottom",
        strip.text.x = element_text(angle = 0),
        axis.title.y = element_text(angle = 0)) +
  scale_fill_grey(name="Project Year",labels=c("2013-2014", "2014-2015")) 
```

```{r time,fig.cap = "Time on Task", fig.width = 10, fig.height = 6}
CLA %>%
  filter(discipline %in% p.select) %>% 
  mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>% 
  select(discipline, project_year,time_pt,time_sr) %>%
  gather(Task, Time, -project_year, -discipline) %>% 
  ggplot(aes(x = Time)) +
  geom_density(aes(fill = Task), position = position_dodge(), alpha=0.5) +
  xlab("\nTime on Task") +
  ylab("Density \n") +
  facet_wrap(~project_year) +
  theme_tufte(base_size = 20) +
  theme(legend.position = "bottom") +
  scale_fill_grey(labels=c("Performance Task", "Selected Response Questions"))
```

\newpage

## Performance Task & Selected Response Scores
The total score is comprised of the scores from the two tasks: the performance task and the selected response questions.  The scores from each represent how well students perform on each task, presented on an absoulte scale, similar to the total score of the CLA+.  The results are presented below as a line graph to illustrate increases or decreases over time.  The blue line and numbers illustrate the mean score.
```{r PT_SR_Scores, fig.width = 16, fig.height = 7,  fig.cap = paste0(params$dept_title, ": CLA+ PT and SR Scores"),  fig.fullwidth = TRUE}

data <- CLA %>%
   group_by(studentid) %>% 
   filter(discipline %in% p.select,!is.na(score_total)) %>%
   mutate(project_year = as.numeric(project_year)) %>% 
   select(studentid, project_year, score_pt, score_sr) %>% 
   rename("Selected Response Score" = score_sr,
          "Performance Task Score" = score_pt) %>% 
   filter(n()>1) %>% 
   ungroup %>% 
   gather(task, score, `Performance Task Score`, `Selected Response Score`) %>% 
   spread(project_year,score) %>% 
   rename(score_y1 = `1`,
          score_y2 = `2`)
  

summary.data <- data %>% 
  group_by(task) %>% 
  summarize_each(funs(mean), -studentid, -task)  %>% 
  mutate_each(funs(floor), -task)

  ggplot(data) +
  geom_segment(aes(group = studentid, x = 1.02, xend = 1.98, y = score_y1, yend = score_y2), alpha = 0.2) +
  geom_segment(aes(x = 1.02, xend = 1.98,  y = score_y1, yend = score_y2), size = 1, color = "blue", summary.data) +
  geom_text(aes(label = score_y1, x = 1, y = score_y1), color = "blue", fontface = "bold", summary.data) +
  geom_text(aes(label = score_y2, x = 2, y = score_y2), color = "blue", fontface = "bold", summary.data) +
  xlab("\nProject Year") +
  ylab(str_wrap("Absolute Score",5)) +
  facet_grid(task~., labeller = label_wrap_gen(width=10)) +
  scale_x_continuous(limits = c(1,2), breaks = c(1,2), labels = c("2013-2014","2014-2015")) +
  geom_rangeframe() +
  theme_tufte(base_size = 20) +
  theme(legend.position = "none",
        axis.title.y = element_text(angle = 0),
        axis.ticks.x = element_blank(),
        strip.text.y = element_text(angle = 0)) 

```

\pagebreak

## Sub-scale Scores
Within each of these tasks, students are also evaluated on how well they display distinct skills.  Each task assesses different skills, and in a different manner. The performance task assesses, using a 6 levelled analytic rubric[^clarubric]:

* Analytical Reasoning and Problem Solving
* Writing Mechanics
* Writing Effectiveness

The results are presented below as horizontal histograms, allowing the comparison of distrbutions over time.

```{r PT_Subscores, fig.width = 10, fig.height = 6,  fig.cap = paste0(params$dept_title, ": CLA+ PT Sub-scale Scores"),  fig.fullwidth = TRUE}

data <- CLA %>%
   filter(discipline %in% p.select,!is.na(score_total)) %>%
   mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>%
   rename(
   "Analytical Reasoning & Problem Solving" = pt_aps,
   "Writing Mechanics" = pt_wm,
   "Writing Effectiveness" = pt_we
   ) %>%
    gather(task, score, `Analytical Reasoning & Problem Solving`, `Writing Mechanics`, `Writing Effectiveness`) 

  
max <- data %>%
  group_by(project_year, task, score) %>% tally %>% ungroup %>% select(n) %>% max() 
      
ggplot(data,aes(x = score)) +
   geom_bar(aes(fill = task), position = position_dodge(), binwidth = 0.5) +
   coord_flip() +
   xlab("Rubric Level\n") +
   ylab("\nCount") +
   geom_hline(yintercept = seq(0, max, 5), col = "white", lwd = 1) +
   facet_grid(task ~ project_year, labeller = label_wrap_gen(width=10)) +
   theme_tufte(base_size = 16) +
   theme(legend.position = "none",
         strip.text.y = element_text(angle = 0),
         axis.title.y = element_text(angle = 0)) +
   scale_x_discrete(breaks = c(1,2,3,4,5,6)) +
   scale_fill_grey(name = "PT Sub-score", labels = str_wrap(c("Analytical Reasoning & Problem Solving", "Writing Mechanics", "Writing Effectiveness"), 10)) 

```
\newpage
While the Selected Response Questions assesses, using an criterion referenced absolute scale:

* Scientific and Quantitative Reasoning
* Critical Reading and Evaluation
* Critiquing an Argument

The results are presented below as a line graph to illustrate increases or decreases over time.  The blue line and numbers illustrates the mean score.
```{r SR_Subscores, fig.width = 12, fig.height = 6,  fig.cap = paste0(params$dept_title, ": CLA+ SR Sub-scale Scores"),  fig.fullwidth = TRUE}

data <- CLA %>%
  filter(discipline %in% p.select,!is.na(sr_sqr),!is.na(sr_ca),!is.na(sr_cre)) %>%
  mutate(project_year = as.numeric(project_year)) %>%
  select(studentid, project_year, sr_sqr, sr_ca, sr_cre) %>% 
  group_by(studentid) %>% 
  filter(n()>1) %>% 
  ungroup %>% 
  rename(
   "Scientific & Quantitative Reasoning" = sr_sqr,
   "Critique an Argument" = sr_ca,
   "Critical Reading & Evaluation" = sr_cre
   ) %>%
   gather(
   task, score, `Scientific & Quantitative Reasoning`, `Critique an Argument`, `Critical Reading & Evaluation`
   ) %>% 
  spread(project_year,score) %>% 
  rename( score_y1 = `1`,
          score_y2 = `2`)

summary.data <- data %>% 
  group_by(task) %>% 
  summarize_each(funs(mean), -studentid, -task)  %>% 
  mutate_each(funs(floor), -task)

ggplot(data) +
  geom_segment(aes(group = studentid, x = 1.02, xend = 1.98, y = score_y1, yend = score_y2), alpha = 0.2) +
  geom_segment(aes(x = 1.02, xend = 1.98,  y = score_y1, yend = score_y2), size = 1, color = "blue", summary.data) +
  geom_text(aes(label = score_y1, x = 1, y = score_y1), color = "blue", fontface = "bold", summary.data) +
  geom_text(aes(label = score_y2, x = 2, y = score_y2), color = "blue", fontface = "bold", summary.data) +
  xlab("\nProject Year") +
  ylab(str_wrap("Absolute Score",5)) +
  facet_grid(task ~ ., labeller = label_wrap_gen(width=10)) +
  scale_x_continuous(limits = c(1,2), breaks = c(1,2), labels = c("2013-2014","2014-2015")) +
  theme_tufte(base_size = 16) +
  theme(legend.position = "none",
        axis.title.y = element_text(angle = 0),
        axis.ticks.x = element_blank(),
        strip.text.y = element_text(angle = 0)) 

```

\newpage

## Critical Thinking Assessment (CAT) Test Results

## Overall Score
As previously stated, the CAT assesses a consensus core set of five skills that address elements of critical thinking, problem solving, and communication across a variety of disciplines. The test only reports a single score on an asbolute scale, which repsents a holistic measurment of the students command and demonstration of these skills in concert.  The results are presented as a line to illustrate student development over time where possible.  In the event of only a single year being available, a histogram of scores are presented.  Blue values indicate the mean.

```{r CAT_scores, fig.width = 10, fig.height = 6,  fig.cap = paste0(params$dept_title, ": CAT Scores"),  fig.fullwidth = TRUE}

if(nrow(CAT %>%
        filter(discipline %in% p.select,!is.na(score)) %>%
        group_by(project_year) %>%
        tally) < 2)
        {
        data <- CAT %>%
        filter(discipline %in% p.select,!is.na(score)) %>%
        select(studentid, project_year, score) %>% 
        mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015")))
        
        mean <-
        data %>% summarize(score = floor(mean(score))) %>% .$score
        
        data <- data %>%
        mutate(fill = ifelse(score == mean,"blue","grey50"))
        
        ggplot(data, aes(x = factor(trunc(score)))) +
        geom_bar(aes(fill = fill)) +
        coord_flip() +
        xlab("\nCAT Score") +
        ylab(str_wrap("Counts", 5)) +
        scale_fill_identity() +
        facet_wrap(~project_year) +
        theme_tufte(base_size = 16) +
        theme(
        legend.position = "none",
        axis.title.y = element_text(angle = 0),
        strip.text.y = element_text(angle = 0)
        )
        } else {
        data <- CAT %>%
        filter(discipline %in% p.select,!is.na(score)) %>%
        mutate(project_year = as.numeric(project_year)) %>%
        select(studentid, project_year, score) %>%
        group_by(studentid) %>%
        filter(n() > 1)
        
        summary.data <- data %>%
        group_by(project_year) %>%
        summarize(score = floor(mean(score)))
        
        ggplot(data, aes(x = project_year, y = score)) +
        geom_path(aes(group = studentid, x = project_year, y = score), alpha = 0.2) +
        geom_path(aes(group = 1), color = "blue", summary.data) +
        geom_point(size = 8, color = "white") +
        geom_point(size = 8, color = "white", data = summary.data) +
        geom_text(
        aes(label = score), size = 3, data %>% group_by(project_year) %>% filter(score != summary.data$score) %>% distinct(score)
        ) +
        geom_text(aes(label = score), color = "blue", fontface = "bold", summary.data) +
        scale_x_continuous(
        limits = c(1,2), breaks = c(1,2), labels = c("2013-2014","2014-2015")
        ) +
        xlab("\nProject Year") +
        ylab(str_wrap("CAT Score", 5)) +
        theme_tufte(base_size = 16) +
        geom_rangeframe() +
        theme(
        legend.position = "none",
        axis.title.y = element_text(angle = 0),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_blank(),
        strip.text.y = element_text(angle = 0)
        )
        }
        
```

## Confounding Factors
Similar to the CLA, there are many reasons that influence student performance.\marginnote{During the first two years of the project, only a single form of the CAT existed, so students would repeat the exact task as they did in the previous years.  This can increase test fatigue, which can negatively effect student motivation.}  This test was embedded within a course, but their performance on the test did not affect their course grades.  They were simply taking the test for participation credit.  This introduces questions regarding the effort students put into the test, and their motivation.  

\newpage

## VALUE Rubric Results

The VALUE Rubrics were used to assess artifacts drawn from courses in the first two years of the program. The specific courses were selected as they represented courses from the `r params$dept_title` that would naturally provide students with the opportunity to exhibit Critical Thinking, Problem Solving and Written Communication.  Each of these artifacts also represent summative displays of student performance. 

```{r VALUE_artifacts}

VALUE %>% 
   filter(discipline %in% p.select) %>%
   select(project_year, semester, subject, course, artifact) %>% 
   unite(course_code, subject, course, sep = " ") %>% 
   mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015")),
          semester = ifelse(semester %in% c(1,3,5,7), "Fall", "Winter")) %>%  
  rename(`Project Year` = project_year,
         `Course` = course_code,
         `Semester` = semester,
         `Artifact` = artifact) %>% 
   distinct %>% 
  kable
```

In order to have the data reflect the abiities of students accurately, we have presented the time frame of each year by semester.  For the `r params$dept_title`, the APSC 200 offering was during the Fall Semester of the second year of the study (and studetns progress through the program).  All of the artifacts assessed were group submissions.  This means that each member of the group received the same score as the others, as there was no way to distinguish individual contributions from the group report.

\newpage

## Overall VALUE Rubric Performance

The results are presented as a slopegraph, a chart which is used compare changes over time for categorical data located on an ordinal or interval scale.  Note that within categories the scale is relative, to depict trends within each category and readily compare it to another.  These results give an overal sense of how students are progressing in these skills within a year, and as they progress through the programs.  

The numbers presented in the graph represent the mean performance of students on critical thinking, problem solving and written communication, by the process below: 

1. Convert ordinal ranking to numeric value
2. Calculate the mean over each sub-scale (row of the rubric)
3. Calculate the mean for the `r params$dept_title`


```{r VALUE_scores, fig.width = 10, fig.height = 6,  fig.cap = paste0(params$dept_title, ": VALUE Scores"),  fig.fullwidth = TRUE}

devtools::source_url("https://raw.githubusercontent.com/jkaupp/r-slopegraph/master/slopegraph.r")

data <- VALUE %>%
  filter(discipline %in% p.select, level!="Not Assessed") %>%
  mutate(project_year = as.numeric(project_year),
         semester = ifelse(course %in% c(101, 100, 104), 1, ifelse(course == 103, 2, ifelse(params$dept %in% c("MINE","CIVL","MTHE","ENPH","ECE","MECH","PSYC","DRAM") & course %in% c(200, 203), 3, 4)))) %>% 
  select(-c(project_year, course, subject, consent, discipline)) %>% 
  mutate(level = as.numeric(level)) 
  
summary.data <- data %>% 
  group_by(rubric, semester) %>% 
  summarize_each(funs(mean), -studentid, -rubric, -dimension, -artifact) %>% 
  ungroup %>% 
  arrange(level) 
 

build_slopegraph(summary.data, x="semester", y="level", group="rubric", method="tufte", min.space=0.05) %>% 
  mutate(x = factor(x, levels = c(1,2,3), labels = str_wrap(c("Fall 2013-2014","Winter 2013-2014","Fall 2014-2015"), 6)),
         y = round(y,1),
         group = str_wrap(group, 8),
         colour = "grey80",
         label.colour = "black") %>% 
  plot_slopegraph(4) +
  theme_tufte(base_size=16, ticks=F) + theme(axis.title=element_blank())

```
\newpage
## VALUE Rubric Sub-scales

The individual VALUE rubric sub scores reveal the strengths and weakeness of students on the invidual elements that comprise Critical Thinking, Problem Solving or Written Communication.  The results are presented as horizontal histograms, which illustrate positive growth by moving rightward and upward.  

```{r VALUE_subscores, fig.width = 16, fig.height = 15,  fig.cap = paste0(params$dept_title, ": VALUE Rubric Sub Scores"),  fig.fullwidth = TRUE}

data <- VALUE %>%
  filter(discipline %in% p.select,!is.na(level)) %>%
  mutate(semester = factor(semester, levels = c(1,2,3,4), labels = c("Fall 2013-2014","Winter 2014-2015","Fall 2014-2015","Winter 2015-2016"))) %>% 
  select(-c(course, subject, consent, discipline)) 

max <- data %>%
  group_by(semester, rubric, dimension, level) %>% tally %>% ungroup %>% select(n) %>% max() 

ggplot(subset(data, rubric == "Critical Thinking"), aes(x = factor(level))) +
  geom_bar(fill = "grey50", position = position_dodge(), binwidth = 0.5) +
  coord_flip() +
  xlab("Level\n") +
  ylab("\nCount") +
  ggtitle("VALUE Rubric: Critical Thinking") +
  geom_hline(yintercept = seq(0, max, 5), col = "white", lwd = 1) +
  facet_grid(dimension ~ semester, labeller = label_wrap_gen(width=10)) +
  theme_tufte(base_size = 20) +
  theme(legend.position = "none",
        strip.text.y = element_text(angle = 0),
        axis.title.y = element_text(angle = 0)) +
  scale_x_discrete(limits = c("Not Assessed", "Below Benchmark 1", "Benchmark 1", "Milestone 2", "Milestone 3", "Capstone 4"))

ggplot(subset(data, rubric == "Problem Solving"), aes(x = factor(level))) +
  geom_bar(fill = "grey50", position = position_dodge(), binwidth = 0.5) +
  coord_flip() +
  xlab("Level\n") +
  ylab("\nCount") +
  ggtitle("VALUE Rubric: Problem Solving") +
  geom_hline(yintercept = seq(0, max, 5), col = "white", lwd = 1) +
  facet_grid(dimension ~ semester, labeller = label_wrap_gen(width=10)) +
  theme_tufte(base_size = 20) +
  theme(legend.position = "none",
        strip.text.y = element_text(angle = 0),
        axis.title.y = element_text(angle = 0)) +
  scale_x_discrete(limits = c("Not Assessed", "Below Benchmark 1", "Benchmark 1", "Milestone 2", "Milestone 3", "Capstone 4"))

ggplot(subset(data, rubric == "Written Communication"), aes(x = factor(level))) +
  geom_bar(fill = "grey50", position = position_dodge(), binwidth = 0.5) +
  coord_flip() +
  xlab("Level\n") +
  ylab("\nCount") +
  ggtitle("VALUE Rubric: Written Communication") +
  geom_hline(yintercept = seq(0, max, 5), col = "white", lwd = 1) +
  facet_grid(dimension ~ semester, labeller = label_wrap_gen(width=10)) +
  theme_tufte(base_size = 20) +
  theme(legend.position = "none",
        strip.text.y = element_text(angle = 0),
        axis.title.y = element_text(angle = 0)) +
  scale_x_discrete(limits = c("Not Assessed", "Below Benchmark 1", "Benchmark 1", "Milestone 2", "Milestone 3", "Capstone 4"))

```
\newpage

## Confounding Factors
The VALUE rubrics do not suffer from the same issues that standardized tests do.  The VALUE rubrics are used on artifacts that are already being submitted as part of a course, so student motivation is tied to their course and program completion, rather than a participation mark.  Used in this fashion, they offer a valid, discipline specific and authentic way to assess esssential learning outcomes in students.

However, there are certain issues that one should be aware of:

* The VALUE rubrics are very dependent upon the structure task and its contextual information, that is to say the results can be skewed if the task is not designed in such a way to allow students to demonstrate the outcomes by which they are assessed.  
* The results are also sensitive to the amount of scaffolding provided within the questions, as some tasks may artificially allow students to achieve a specific level by design, not by performance.
* The results presented have reduced variance due to the group nature of the artifacts.  If individual reports were used, there would most likely be a greater distribution and variance in the results.

## Transferable Learning Orienations Survey Results

The results of the TLO are presented below. When the survey was administered, along with the number of responses is illustrated in the table below.

```{r, tlo table}
TLO %>% 
    filter(discipline %in% p.select) %>% 
    mutate(project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>% 
    group_by(project_year, course) %>% 
    mutate(n = n()) %>% 
    select(project_year, course, n) %>% 
    rename(`Project Year` = project_year,
           Course = course,
           N = n) %>% 
    distinct %>% 
    kable
```

There may be limited responses to the TLO in second year of the project, so in certain departments no connection can be shown that illustrates the changes that occur between first and second year. The results are also from a previous version of the TLO, which include Self-efficacy.  The current version of the instrument has merged Self-efficacy with Organization, and added additional items to other scales.

When interpreting these results keep in mind that respondents will answer surveys with an ideal version of themselves in mind.  This results in a high rating for the self-rating questions (which provides them the opportunity to indicate what is the most like them).  The scale items provide a less biased representation of the student by aggregating the discrete aspects of the scale into a whole.  The interesting aspect to note is the difference between the two.  By looking at the differences, programs can see the types of students that are entering their program and use the information to:

* Provide targeted support in gap areas
* Provide to instructors to adapt teaching strategies
* Identify how programming and curriculum can improve these aspects in their students.

Ideally, should longitudinal data for this survey exist programs could use it to see how their program influences these aspects of students, and how they can improve to better support and develop students as they complete the progam.

These results presented below are horizontal histograms aggregate of all students, paneled by year when possible.  The charts contrast each of the scale items of the TLO showing the instrument calculated score of the student (mean score of scale items), alongside their self rating. 

```{r TLO_results, fig.width = 16, fig.height = 20,  fig.cap = paste0(params$dept_title, ": Transferrable Learning Orientations Results"),  fig.fullwidth = TRUE, results = "asis", comment = NA}
library(scales)

scale.data <- TLO %>% 
  filter(discipline %in% p.select) %>%
  select_(., .dots = as.list(names(.[vapply(., is.numeric, logical(1))]))) %>% 
  gather(item, value, -studentid, -project_year, -year) %>% 
  mutate(scale = substr(item, 0,2),
         project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>% 
  filter(!str_detect(item, "(\\w+6)")) %>% 
  group_by(studentid, project_year, scale) %>% 
  filter(!is.na(value)) %>% 
  summarize(mean = mean(value)) %>% 
  mutate(mean = rescale(mean, to = c(1,4))) %>% 
  mutate(mean = as.numeric(cut(mean,c(0,1,2,3,4), labels=c(1,2,3,4))))

selfrated.data <- TLO %>% 
  filter(discipline %in% p.select) %>%
  select_(., .dots = as.list(names(.[vapply(., is.numeric, logical(1))]))) %>% 
  gather(item, value, -studentid, -project_year, -year) %>% 
  mutate(scale = substr(item, 0,2),
         project_year = factor(project_year,c(1,2), c("2013-2014","2014-2015"))) %>% 
  filter(str_detect(item, "(\\w+6)")) %>% 
  group_by(studentid,project_year,scale) %>% 
  filter(!is.na(value)) %>% 
  summarize(self_rating = floor(mean(value))) 

# Data for histograms below:
data <- left_join(scale.data, selfrated.data) %>%
  # mutate(diff = as.numeric(mean) - self_rating) %>% 
  mutate(scale = factor(scale,labels = c(
   "OM" = "Outcome Motivation",
   "OR" = "Organization",
   "SE" = "Self-efficacy",
   "LB" = "Learning Belief",
   "SR" = "Self Regulation",
   "TR" = "Transfer")
  )) %>%
  gather(rating, value, -scale, -studentid, -project_year) 
  
om_resp <- 
  c(
  "My main motivation for learning is to avoid letting others down or being seen as a failure.",
  "My main motivation is academic success, but I still enjoy exploring the subject matter.",
  "My motivation to understand the subject matter is as important to me as my academic success.",
  "I am motivated by intense curiosity to understand subject matter; academic success is a secondary concern."
  )

lb_resp <-
  c(
  "My academic performance is dependent on factors I cannot control.",
  "I have some control over the factors affecting my academic performance.",
  "I am adaptable and can control most of the factors affecting my academic performance.",
  "I am capable of changing my thinking or approach to learning; I have total control over my academic performance."
  )

se_resp <-
  c(
  "I feel tentative about having the knowledge/ skills I needed to meet the course requirements.",
  "I feel confident that my knowledge/ skills are adequate to meet most of the course requirements.",
  "I feel confident that I have the expected level of knowledge/ skills to meet all of the course requirements.",
  "I feel very confident that I have all of the necessary knowledge/ skills to excel in meeting all of the course requirements."
  )

tr_resp <-
  c(
  "I preferred to memorize key information as it pertained to a specific subject, but sometimes thought about where learned similar things.",
  "I preferred to memorize key information, but also related ideas to what I had learned previously.",
  "I consistently connected new ideas to what I already know when I applied my knowledge and skills in my courses.",
  "I made meaningful connections between new and previous learning when I applied knowledge and skills creatively across my courses."
  )
or_resp <-
  c(
"My organization was inconsistent; I changed plans regularly to suit my personal needs.",
"I tried to organize my work processes, and attempted to structure my study schedules according to deadlines.",
"I actively organized my schedules according to my short-term and long-term goals.",
"I strategically managed my schedules, and made necessary accommodations, in prioritizing to meet all of my goals."
)

sr_resp <-
  c(
    "I work to the best of my ability, even when I don’t always know exactly what is required.",
"I make sure I know what I need to do, and try different strategies meet course requirements.",
"I always direct my attention appropriately and manage my learning to meet course requirements.",
"I excel in my courses by systematically structuring my learning, compensating for any gaps in knowledge or skills."
)

tlo_item_hist <- function(df, items, legend)
{
ggplot(df) +
  geom_bar(aes(x = factor(value), fill = factor(rating)), position = position_dodge(), binwidth = 0.5) +
  geom_hline(yintercept = seq(0, max(table(df$value)), 5), col = "white", lwd = 1) +
  geom_text(aes(x = factor(value), y = ..count.., label = ..count.., fill = factor(rating)), stat = "bin", position = position_dodge(width=0.9), hjust = -1) +
  coord_flip() +
  scale_x_discrete(labels = str_wrap(items, 50)) +
  scale_fill_grey(name ="", labels = c("Scale Rating", "Self Rating")) +
  xlab(NULL) +
  ylab("Count") +
  facet_wrap(~project_year) +
  ggtitle(as.character(unique(df$scale))) +
  theme_tufte(base_size = 16) +
  theme(legend.position = legend,
        strip.text.y = element_text(angle = 0),
        axis.title.y = element_text(angle = 0),
        axis.text.y  = element_text(size = rel(1.5)),
        axis.ticks.y = element_blank())
}

SR <- tlo_item_hist(subset(data, scale=="Self Regulation"), sr_resp, "none")
OM <- tlo_item_hist(subset(data, scale=="Outcome Motivation"), om_resp, "none")
TR <- tlo_item_hist(subset(data, scale=="Transfer"), tr_resp, "bottom")
OR <- tlo_item_hist(subset(data, scale=="Organization"), or_resp, "none")
SE <- tlo_item_hist(subset(data, scale=="Self-efficacy"), se_resp, "none")
LB <- tlo_item_hist(subset(data, scale=="Learning Belief"), lb_resp, "bottom")

plots1 <- list(SR,OM,TR)

plots2 <- list(OR,SE,LB)

grid.arrange(
    grobs = plots1, ncol = 1)

grid.arrange(
    grobs = plots2, ncol = 1)
    
```

## Confounding Factors

As with all surveys, this is qualitative and self-reported.  However, the TLO has been developed and tested to ensure psychometic validity and reliability and future versions and improvements are being considered. 


[^heqco]: http://www.heqco.ca
[^qlop]: http://www.queensu.ca/qloa/home
[^tools]: http://www.queensu.ca/qloa/assessment-tools
[^cla+]: http://cae.org/
[^cat]: http://www.tntech.edu/cat
[^value]: http://www.aacu.org/value/rubrics
[^tlo]: http://www.queensu.ca/qloa/assessment-tools/transferable-learning-orientations-tlo-survey
[^mastery]: bit.ly/intrp_cla_results
[^clarubric]: http://cae.org/images/uploads/pdf/CLA_Plus_Practice_PT.pdf